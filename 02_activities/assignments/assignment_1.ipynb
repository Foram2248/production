{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `01_materials/labs/2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below.\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:07,183, 608807953.py, 15, INFO, Loaded PRICE_DATA: ../../05_src/data/prices/\n",
      "2025-06-08 21:13:07,269, 608807953.py, 20, INFO, Found3014 parquet files.\n",
      "2025-06-08 21:13:07,270, 608807953.py, 23, INFO, First 5 parquet files:['../../05_src/data/prices/NXJ/NXJ_2007/part.0.parquet', '../../05_src/data/prices/NXJ/NXJ_2007/part.1.parquet', '../../05_src/data/prices/NXJ/NXJ_2009/part.0.parquet', '../../05_src/data/prices/NXJ/NXJ_2009/part.1.parquet', '../../05_src/data/prices/NXJ/NXJ_2008/part.0.parquet']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Write your code below.\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "from utils.logger import get_logger\n",
    "_logs = get_logger(__name__)\n",
    "\n",
    "\n",
    "# Load the environment variable PRICE_DATA\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "_logs.info(f\"Loaded PRICE_DATA: {PRICE_DATA}\")\n",
    "\n",
    "# Using glob to find the path of all parquet files\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive=True)\n",
    "\n",
    "_logs.info(f\"Found{len(parquet_files)} parquet files.\")\n",
    "\n",
    "#Verification\n",
    "_logs.info(f\"First 5 parquet files:{parquet_files[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Close:\n",
    "    \n",
    "    - `returns`: (Close / Close_lag_1) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:15,986, 3430312111.py, 5, INFO, Loaded parquet files into Dask Dataframe (dd_px).\n",
      "2025-06-08 21:13:15,987, 3430312111.py, 6, INFO, dd_px dtypes: Date          datetime64[ns]\n",
      "Open                 float64\n",
      "High                 float64\n",
      "Low                  float64\n",
      "Close                float64\n",
      "Adj Close            float64\n",
      "Volume               float64\n",
      "source       string[pyarrow]\n",
      "Year                   int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Setting up ticker as index \n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")\n",
    "_logs.info(\"Loaded parquet files into Dask Dataframe (dd_px).\")\n",
    "_logs.info(f\"dd_px dtypes: {dd_px.dtypes}\") # Check dtypes for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:21,122, 1134874372.py, 6, INFO, 'Date' column is already datetime64[ns] type.\n"
     ]
    }
   ],
   "source": [
    "# 'Date'  is in date time but if its not then we conver from obj to DateTime so lag operation runs smoothly\n",
    "if 'Date' in dd_px.columns and dd_px['Date'].dtype != 'datetime64[ns]':\n",
    "    dd_px['Date'] = dd_px['Date'].astype('datetime64[ns]')\n",
    "    _logs.info(\"Converted 'Date' column to datetime64[ns].\")\n",
    "else:\n",
    "      _logs.info(\"'Date' column is already datetime64[ns] type.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:29,099, 501825442.py, 2, INFO, Starting feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Adding features on dd_px\n",
    "_logs.info(\"Starting feature engineering\")\n",
    "\n",
    "def add_features(group):\n",
    "    \"\"\"Function to add features to each ticker group\"\"\"\n",
    "\n",
    "    # Sort by date to ensure proper lag calculations\n",
    "    group = group.sort_values('Date')\n",
    "\n",
    "    # Add lag feature\n",
    "    group = group.assign(\n",
    "        Close_lag_1 = group['Close'].shift(1),\n",
    "        Adj_Close_lag_1 = group['Adj Close'].shift(1)\n",
    "    )\n",
    "\n",
    "    # Add returns\n",
    "    group = group.assign(\n",
    "        returns = (group['Close']/group['Close_lag_1']) - 1\n",
    "    )\n",
    "\n",
    "    # Add hi_lo range\n",
    "    group = group.assign(\n",
    "        hi_lo_range = group['High'] - group['Low']\n",
    "    )\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:32,737, 2020478666.py, 2, INFO, Creating meta specification\n",
      "2025-06-08 21:13:36,879, 2020478666.py, 10, INFO, Meta specification created with 14 columns\n"
     ]
    }
   ],
   "source": [
    "# Get a small sample to create proper meta specification\n",
    "_logs.info(\"Creating meta specification\")\n",
    "sample_data = dd_px.head(100).reset_index()\n",
    "sample_ticker = sample_data['ticker'].iloc[0]\n",
    "sample_group = sample_data[sample_data['ticker']==sample_ticker]\n",
    "sample_with_features = add_features(sample_group)\n",
    "\n",
    "# Create meta dictionary based on sample\n",
    "meta_dict = {col:sample_with_features[col].dtype for col in sample_with_features.columns}\n",
    "_logs.info(f\"Meta specification created with {len(meta_dict)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:42,856, 1209356697.py, 2, INFO, Applying feature engineering to all ticker groups\n",
      "2025-06-08 21:13:42,865, 1209356697.py, 9, INFO, Successfully added all features:\n",
      "2025-06-08 21:13:42,866, 1209356697.py, 10, INFO, - Close_lag_1: Previous day's closing price\n",
      "2025-06-08 21:13:42,866, 1209356697.py, 11, INFO, - Adj_Close_lag_1: Previous day's adjusted closing price\n",
      "2025-06-08 21:13:42,867, 1209356697.py, 12, INFO, - returns: (Close / Close_lag_1) - 1\n",
      "2025-06-08 21:13:42,868, 1209356697.py, 13, INFO, - hi_lo_range: High - Low\n",
      "2025-06-08 21:13:42,869, 1209356697.py, 15, INFO, dd_feat columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source', 'Year', 'Close_lag_1', 'Adj_Close_lag_1', 'returns', 'hi_lo_range']\n"
     ]
    }
   ],
   "source": [
    "# Sample meta specificaiton worked sucesfully so we can now apply our group funtion to the whole data\n",
    "_logs.info(\"Applying feature engineering to all ticker groups\")\n",
    "dd_feat = dd_px.reset_index().groupby('ticker', group_keys=False).apply(\n",
    "    add_features,\n",
    "    meta=meta_dict\n",
    ")\n",
    "# Set ticker back as index if needed\n",
    "dd_feat = dd_feat.set_index('ticker')\n",
    "_logs.info(\"Successfully added all features:\")\n",
    "_logs.info(\"- Close_lag_1: Previous day's closing price\")\n",
    "_logs.info(\"- Adj_Close_lag_1: Previous day's adjusted closing price\") \n",
    "_logs.info(\"- returns: (Close / Close_lag_1) - 1\")\n",
    "_logs.info(\"- hi_lo_range: High - Low\")\n",
    "\n",
    "_logs.info(f\"dd_feat columns: {dd_feat.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a new feature containing the moving average of `returns` using a window of 10 days. There are several ways to solve this task, a simple one uses `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:13:48,418, 555958015.py, 3, INFO, Converting Dask dataframe to pandas dataframe\n",
      "2025-06-08 21:14:12,296, 555958015.py, 5, INFO, Converted to pandas dataframe with shape: (346778, 13)\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "_logs.info(\"Converting Dask dataframe to pandas dataframe\")\n",
    "df_feat = dd_feat.compute() # pandas \n",
    "_logs.info(f\"Converted to pandas dataframe with shape: {df_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:14:15,598, 3816887444.py, 2, INFO, Adding 10-day moving average of returns\n",
      "2025-06-08 21:14:15,709, 3816887444.py, 8, INFO, Added 'returns_ma_10' feature (10-day moving average of returns).\n",
      "2025-06-08 21:14:15,709, 3816887444.py, 9, INFO, Final dataframe shape: (346778, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment completed successfully!\n",
      "Final dataframe shape: (346778, 14)\n",
      "Number of columns: 14\n"
     ]
    }
   ],
   "source": [
    "# Adding moving average of retuns with 10-day window\n",
    "_logs.info(\"Adding 10-day moving average of returns\")\n",
    "\n",
    "df_feat = df_feat.sort_values(['ticker', 'Date']) \n",
    "\n",
    "df_feat['returns_ma_10'] = df_feat.groupby('ticker')['returns'].transform(lambda x:x.rolling(10).mean()) # pandas \n",
    "\n",
    "_logs.info(\"Added 'returns_ma_10' feature (10-day moving average of returns).\")\n",
    "_logs.info(f\"Final dataframe shape: {df_feat.shape}\")\n",
    "\n",
    "print(\"Assignment completed successfully!\")\n",
    "print(f\"Final dataframe shape: {df_feat.shape}\")\n",
    "print(f\"Number of columns: {len(df_feat.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 21:14:20,129, 1889327385.py, 4, INFO, Sample of returns_ma_10 by ticker:\n",
      "             Date    Close  returns  returns_ma_10\n",
      "ticker                                            \n",
      "A      1999-12-03  31.8312   0.0085         0.0024\n",
      "A      1999-12-06  32.7253   0.0281         0.0135\n",
      "A      1999-12-07  32.3677  -0.0109         0.0034\n",
      "ACB    2000-01-18  39.0312  -0.0068        -0.0045\n",
      "ACB    2000-01-19  39.0312   0.0000        -0.0045\n",
      "ACB    2000-01-20  38.7657  -0.0068        -0.0020\n",
      "ACBI   2015-11-16  14.5300  -0.0176        -0.0026\n",
      "ACBI   2015-11-17  14.7500   0.0151        -0.0055\n",
      "ACBI   2015-11-18  14.5700  -0.0122         0.0020\n",
      "ACIO   2019-07-24  25.3200   0.0043         0.0008\n",
      "ACIO   2019-07-25  25.2680  -0.0021         0.0006\n",
      "ACIO   2019-07-26  25.4000   0.0052         0.0007\n",
      "ALDX   2014-05-16   7.2200  -0.0296         0.0036\n",
      "ALDX   2014-05-19   7.4100   0.0263         0.0132\n",
      "ALDX   2014-05-20   7.3400  -0.0094         0.0212\n"
     ]
    }
   ],
   "source": [
    "# Select a nice sample that shows the moving average in action\n",
    "sample_view = df_feat[df_feat['returns_ma_10'].notna()].groupby('ticker').head(3)\n",
    "sample_table = sample_view[['Date', 'Close', 'returns', 'returns_ma_10']].round(4)\n",
    "_logs.info(f\"Sample of returns_ma_10 by ticker:\\n{sample_table.head(15).to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "\n",
    "No, it was not necessary to convert to pandas. It depends on the usecase. Pandas for small to medium datasets, easy inspection, debugging and analysis. Its straight forward in pandas. (like in our case)\n",
    "\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "Dask for large datasets, limited system memory, if it involves many operations before final compute and the availability of distributed computing. Does not use memory until we hit compute(). If we have a need for parallel/distributed computing we go for dask.\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
